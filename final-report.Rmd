---
title: "Untitled"
subtitle: "PHS 7095, Spring 2024 - Final Report"
geometry: margin=3cm
output: 
  pdf_document:
    latex_engine: xelatex    
    number_sections: true
author: "Kline Dubose, Haojia Li"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning = F, message = F, cache = F)
options(knitr.kable.NA = "")
options(digits = 4)
```

# Introduction

- Non-standard because the parameter under the null hypothesis is on the boundary of the parameter space and the response variables are not independent under the alternative.
- Special case of variance component of familial relatedness in linear mixed models.

# Likelihood ratio tests in linear mixed models with one variance component

Crainiceanu and Ruppert (2004) derived both the finite sample distribution and the asymptotic distribution of the LRT and RLRT statistics for testing the null hypothesis that the variance component is 0 in a linear mixed model (LMM) with one variance component. 
They used weak assumptions on eigenvalues of certain design matrices, and released the hypothesis of i.i.d. data in the restrictive assumptions by Self and Liang (1987), that the response variable vector can be partitioned into independent and identically distributed subvectors and the number of independent subvectors tends to $\infty$.

Consider an LMM with one variance component

$$
\mathbf{Y} = X\mathbf{\beta} + Z\mathbf{b} + \epsilon,\
E\left(\begin{array}{c}\mathbf{b}\\\epsilon\end{array}\right) =
\left(\begin{array}{c}\mathbf{0}_K\\\mathbf{0}_n\end{array}\right),\
\text{cov}\left(\begin{array}{c}\mathbf{b}\\\epsilon\end{array}\right) = 
\left(\begin{array}{cc}\sigma_b^2 \Sigma & \mathbf{0}\\\mathbf{0} & \sigma_{\epsilon}^2 I_n\end{array}\right),
\tag{1}
$$

where,

- $\mathbf{Y}$ is the $n \times 1$ vector of observations,
- $X$ is the $n \times p$ design matrix for the fixed effects,
- $Z$ is the $n \times K$ design matrix for the random effects,
- $\beta$ is a $p$-dimensional vector of fixed effects parameters,
- $\mathbf{b}$ is a $K$-dimensional vector of random effects,
- $(\mathbf{b}, \epsilon)$ has a normal distribution.

Under these conditions it follows that

$$
\begin{aligned}
E(\mathbf{Y}) &= X\beta,\\
\text{var}(\mathbf{Y}) &= \sigma_{\epsilon}^2 V_{\lambda}
\end{aligned}
$$

where

- $\lambda = \sigma_b^2 / \sigma_{\epsilon}^2$, which can be considered a signal-to-noise ratio,
- $V_{\lambda} = I_n + \lambda Z\Sigma Z'$.

In this context, $\sigma_b^2 = 0$ if and only if $\lambda = 0$, and the parameter space for $\lambda$ is $[0, \infty)$.

The authors proposed a general form of the null hypothesis ($H_0$) and alternative hypothesis ($H_A$):

$$\begin{aligned}
H_0&: \beta_{p+1-q} = \beta_{p+1-q}^0, ..., \beta_p = \beta_p^0,\ \ \ \ \ \ \ \sigma_b^2 = 0 \text{ (or equivalently }\lambda = 0) \\
H_A&: \beta_{p+1-q} \neq \beta_{p+1-q}^0, ..., \beta_p \neq \beta_p^0,\ \ \ \ \ \ \ \sigma_b^2 > 0 \text{ (or equivalently }\lambda > 0)
\end{aligned}$$

where the $q$ denotes the number of fixed effects parameters constrained under $H_0$. 
An application of this method with $q>0$ is using LRTs and RLRTs for testing a polynomial fit against a general alternative described by penalized splines (P-splines), in Section 5 of the paper. 

However, we will only focus on the particular case of $q=0$:

$$
H_0: \sigma_b^2 = 0\ (\lambda = 0)\ \text{ vs. } \ H_A: \sigma_b^2 > 0\ (\lambda > 0)
$$

Twice the log-likelihood ratio statistic for model (1) is

$$
2\log L(\beta, \lambda, \sigma_e^2) = -\log(\sigma_e^2) - \log |V_\lambda| - \frac{(Y - X\beta)'V_\lambda^{-1}(Y - X\beta)}{\sigma_e^2}
$$



## Algorithm to simulate the null finite sample distribution of $LRT_n$

*Step 1*: define a grid 0 = $\lambda_1$ < $\lambda_2$ < ... < $\lambda_m$ of possible values for $\lambda$.

*Step 2*: simulate K independent $\chi^2_1$ random variables $w^2_1$, ..., $w^2_K$. Set $S_K = \sum^K_{s=1} w_s^2$.

*Step 3*: independently of step 1, simulate $X_{n,K,p} = \sum^{n-p}_{s=K+1} w_s^2$ with a $\chi^2_{n-p-K}$-distribution.

*Step 4*: independently of steps 1 and 2, simulate $X_q = \sum^q_{s=1} u_s^2$ with a $\chi^2_q$-distribution.

*Step 5*: for every grid point $\lambda_i$ compute

$$\begin{gathered}
N_n(\lambda_i) = \sum^K_{s=1} \frac{\lambda_i \mu_{s,n}}{1+\lambda_i \mu_{s,n}} w_s^2 \\
D_n(\lambda_i) = \sum^K_{s=1} \frac{w_s^2}{1+\lambda_i \mu_{s,n}} w_s^2 + X_{n,K,p} \\
\end{gathered}$$

*Step 6*: determine $\lambda_{max}$ which maximizes $f_n(\lambda_i)$ over $\lambda_1$, ..., $\lambda_m$.

*Step 7*: compute 

$$LRT_n = f_n(\lambda_{max}) + n\log (1+ \frac{X_q}{S_K + X_{n,K,p}})$$

*Step 8*: repeat steps 2-7 until the desired number of simulations is achieved.



## Conclusion



# Proofs of Lemma 3, Theorem 1, and Corollary 1

Before we begin, in this scenario, our null hypothesis is $\lambda = 0$ where $\lambda$ is the effect size of familial relatedness (FR) and the alternative is $\lambda > 0$.

## Lemma 3

***Suppose that $sup_{s \in \Omega_0}\{\rho_s\} \le B$ are bounded. Under the null hypothesis, (i) both $F_n(\lambda)$ and $G_n(\lambda)$ uniformly converge in probability to $F_0(\lambda) = \sum_{j=0}^m q_j \log(w_j) - \log(\sum_{j=0}^m q_j w_j$ over $\lambda \in [-\delta, T]$ for any $0 < \delta < \frac{1}{B \vee max_j\{ \phi_j\}}$ and $0 < T < +\infty$; (ii) $F_0(\lambda)$ achieves its unique maximum at $\lambda = 0$; (iii) $\hat \lambda_n \rightarrow^p 0$ and $\hat \lambda^r_n \rightarrow^p 0$.***

i) Essentially we will be showing that:

$$
\begin{split}
F_n(\lambda) \rightarrow^p F_0(\lambda) \\
G_n(\lambda) \rightarrow^p F_0(\lambda)
\end{split}
$$

where:

$$
F_0(\lambda) = \sum_{j=0}^m \log(w_j) - \log\left(\sum_{j=0}^m q_j w_j\right)
$$

Additionally, note that $\phi_j$ is the set of all distictive non-zero eigenvalues of the FR correlation matrix. 

Let's start with rewriting $F_n(\lambda)$:

$$
\begin{split}
F_n(\lambda) & = \log \left( \frac{\sum_{i=1}^{n-p} u_i^2}{\sum_{j=0}^m f'_j w_j U_j + \sum_{s \in \Omega_0} v_s u_s^2} \right) + \frac{1}{n} \sum_{j=1}^m f_j \log (w_j) \\
& = \log \left(\frac{n-p}{n-p} \cdot \frac{\sum_{i=1}^{n-p} u_i^2}{\sum_{j=0}^m f'_j w_j U_j + \sum_{s \in \omega_0} v_s u_s^2} \right) + \frac{1}{n} \sum_{j=1}^m f_j \log (w_j) \\
& = \log\left( \frac{1}{n-p} \sum_{i=1}^{n-p} u_i^2 \right) - \log\left( \frac{\sum_{j=0}^m f'_j w_j U_j}{n-p} + \frac{\sum_{s \in \Omega_0}v_s u_s^2}{n-p}  \right) + \frac 1 n \sum_{j=1}^m f_j \log(w_j) 
\end{split}
$$

Looking at the first term, let's note that $u_i \sim N(0,1)$ for $i = 1, ..., n-p$. 
This allows us to rewrite the first term and note that:

$$
\begin{split}
\frac{1}{n-p} \sum_{i=1}^{n-p} u_i^2 = \frac{1}{n-p} \sum_{i=1}^{n-p} (u_i - 0)^2 = S_u  & \rightarrow^p \sigma_u = 1 \\
\frac{1}{n-p} \sum_{i=1}^{n-p} u_i^2 & \rightarrow^p 1
\end{split}
$$

by the week law of large numbers. 

$$
\Rightarrow \log\left( \frac{1}{n-p} \sum_{i=1}^{n-p} u_i^2 \right) \rightarrow ^p \log(1) = 0
$$

Looking at the second term, we note that $U_j = \frac{1}{f'_j} \sum_{i = \Omega_j} u_i^2$ for $j = 0, ..., m$. Additionally, we note here that $f'_j = |\Omega_j|$ which is the count of the times that $phi_j$ is replicated in non-zero eignevalues. For the purposes of this, it is sufficient to say that $|\Omega_j|$ is a count. 

We can rewrite $U_j$ as:

$$
U_j = \frac{1}{|\Omega_j|} \sum_{i = \Omega_j} (u_i - 0)^2 \rightarrow^p 1
$$

since $u_i \sim N(0,1)$ as previously stated. 

Additionally, this implies that as $U_j \rightarrow^p 1$:

$$
\Rightarrow \sum_{s \in \Omega_0} \frac{v_s u_s^2}{n - p} \rightarrow^p 0
$$

under the null hypothesis. 

This is since $v_s = 1$ under the null hypothesis:

$$
v_s = \frac{1}{1+\lambda \rho_s} = \frac{1}{1+0} = 1
$$

Additionally, as long as $\sum_{s \in \Omega_0} {u_s^2}$ is finite, then $\sum_{s \in \Omega_0} \frac{v_s u_s^2}{n - p} = \sum_{s \in \Omega_0} \frac{u_s^2}{n - p} \rightarrow 0$ as $(n-p) \rightarrow \infty$.

Noted in the paper:

$$
\begin{split}
\frac{f'_j}{n-p} \rightarrow q_j \\
\frac{f_j}{n} \rightarrow q_j
\end{split}
$$

The suggests that the second term of the equation:

$$
- \log\left( \frac{\sum_{j=0}^m f'_j w_j U_j}{n-p} + \frac{\sum_{s \in \Omega_0}v_s u_s^2}{n-p}  \right) \rightarrow^p -\log\left( \sum_{j=0}^m q_j w_j + 0\right) = -\log\left( \sum_{j=0}^m q_j w_j \right)
$$

The third element of the equation converges:

$$
\sum_{j=1}^m \frac{f_j}{n} \log(w_j) \rightarrow^p \sum_{j=1}^m q_j \log(w_j)
$$

Let's now look $G_n(\lambda)$. Let's start by rewriting $G_n(\lambda)$:

$$
\begin{split}
G_n(\lambda) & = \log\left( \frac{\sum_{i=1}^{n-p}u_i^2}{\sum_{j=0}^m f'_j w_j U_j + \sum_{s\in \Omega_0}v_s u_s^2} \right) + \frac{1}{n-p}\sum_{j=1}^m f'_j \log(w_j) + \frac{1}{n-p} \sum_{s\in\Omega_0}\log(v_s) \\
& = \log\left( \frac{n-p}{n-p} \cdot \frac{\sum_{i=1}^{n-p}u_i^2}{\sum_{j=0}^m f'_j w_j U_j + \sum_{s\in \Omega_0}v_s u_s^2} \right) + \frac{1}{n-p} \sum_{j=1}^m f'_j \log(w_j) + \frac{1}{n-p}\sum_{s\in\Omega_0}\log(v_s) \\
& = \log\left(\frac{1}{n-p}\sum_{j=1}^{n-p}u_i^2\right) - \log\left(\frac{1}{n-p}\left(\sum_{j=0}^m f'_j w_j U_j + \sum_{s\in\Omega_0}v_s u_s^2\right)\right) + \sum_{j=1}^m \frac{f'_j}{n-p}\log(w_j) + \frac{\sum_{s\in\Omega_0}\log(v_s)}{n-p}
\end{split}
$$

In the previous proof of $F_n(\lambda)$ we showed the convergence of the first and second elements of $G_n(\lambda)$. 

For the third element, recall that $\frac{f'_j}{n-p} \rightarrow q_j$ which implies that $\sum_{j=1}^m \frac{f'_j}{n-p}\log(w_j) \rightarrow^p\sum_{j=1}^m q_j\log(w_j)$ which has been shown that the supremum of this third element converges in probability to 0.

The final element of this equation converges to $0$, since under the null hypothesis $v_s \rightarrow 1$ as previously shown. 

Therefore by the uniform convergence theorem, $\sup_{\lambda \in [-\delta,T]} \left|G_n(\lambda) - F_0(\lambda)\right| \rightarrow^p 0$. $G_n(\lambda)$ therefore converges uniformly to $F_0(\lambda)$ in probability over $\lambda \in [-\delta, T]$.


ii) Recall that $F_0(\lambda) = \sum_{j=0}^m q_j \log(w_j) - \log\left(\sum_{j = 0}^m q_j w_j\right)$.

The proof that the authors used involved Jensen's inequality to show that $F_0(\lambda)$ achieves its unique maximum at $\lambda = 0$. I was confused by this and will include my thought process in coming to the conclusion that I did.

When evaluating under the null hypothesis at $\lambda = 0$, we really only need to worry about the second term $- \log(\sum_{j=0}^m q_j w_j)$. Additionally, when considering with the constrain of the bounded parameter space $\lambda \ge 0$, we can see that $F_0(\lambda) \le 0$ when $\lambda \ge 0$. We also note that $q_j$ is a proportion such that $\sum_{j=0}^m q_j \le 1$. 

Under these constraints. 

$$
\begin{split}
F_0(0) & = -\log\left( \sum_{j=0}^m q_j \right) | \sum_{j=0}^m q_j \le 1 \\
\Rightarrow F_0(0) & \ge 0 \; when \; \lambda = 0 \\
\end{split}
$$

Since $\lambda$ can only be non-negative with the way it has been defined in this paper:

$$
\begin{split}
\left|\sum_{j=0}^m q_j \log\left(\frac{1}{1 + \lambda \phi_j}\right)\right| > \left| \log\left(\sum_{j=0}^m q_j \frac{1}{1 + \lambda\phi_j}\right) \right| \\
\Rightarrow F_0(\lambda) \le 0
\end{split}
$$

Which suggests that $\lambda = 0$ is the unique maximum of $F_0(\lambda)$.

iii) Since $\lambda = 0$ has been established as the unique global maximum, we can reference theorem 5.7 of *Asymptotic Statistics* (1998) by A. van der Vaart, which states that if there is a unique global estimator with the properties we have already established, then $\hat \theta \rightarrow^p \theta_0$. In this case, both the MLE $\hat \lambda_n$ and the REML $\hat \lambda^r_n$ have the unique global maximu at $\lambda = 0$ and would both converge in probability to $0$ according to this theorem.

## Theorem 1

***Suppose the $sup_{s\in \Omega_0} \le B$ are bounded and $p = o\left(\sqrt{n-p}\right)$. Then, under the null hypothesis (i) $\sqrt{n-p} \hat \lambda_n \rightarrow^p \frac 1 2 1_{\{0\}} + \frac 1 2 N(0, \sigma^2_\lambda)^+$, as $n \rightarrow \infty$; (ii) $\sqrt{n-p} \hat \lambda^r_n \rightarrow^D \frac 1 2 1_{\{0\}} + \frac 1 2 N(0, \sigma_\lambda^2)^+$, as $(n-p) \rightarrow \infty$; where $\sigma_\lambda^2 = \frac{2}{V(Z_\phi)}$ and $N(0, \sigma_\lambda^2)^+ = max(0, N(0, \sigma_\lambda^2))$ is the left truncated distribution of $N(0, \sigma_\lambda^2)$ at 0. ***

As previoiusly discussed in Lemma 3, $\hat \lambda_n$ and $\hat \lambda_n^r$ are the global maximizers under the null over the interval $[-\delta, T]$.

**i)**

Let's think of MLE $\hat \lambda_n$ as the left truncated at 0. Such that:

$$
\hat \lambda_n = \{ \begin{split}
& \tilde \lambda_n \; \; \; \tilde \lambda_n >0 \\
& 0 \; \; \; \; \; \tilde \lambda_n \le 0
\end{split}
$$

When $n$ is large enough, note that $F_n' (\tilde \lambda_n) = 0$.

If we take the Taylor series expansion of $F_n'$ around $0$ we get:

$$
0 = F_n'(0) + \tilde \lambda_n F_n''(0) + \frac 1 2 \tilde \lambda^2_n F_n''(c_n\tilde\lambda_n)
$$
where $c \in [0,1]$ and we define $c_n\tilde\lambda_n = \eta_n$

We can rewrite the Taylor expansion.

$$
\begin{split}
-F_n'(0) & = \tilde \lambda_n F_n''(0) + \frac 1 2 \tilde \lambda_n^2 F_n'''(\eta_n) \\
\Rightarrow \tilde \lambda_n(F_n''(0) + \frac 1 2 \tilde \lambda F_n'''(\eta_n))& = -F_n'(0) \\
\Rightarrow \sqrt{n-p} \tilde \lambda_n & = \frac
{\sqrt{n-p}F_n'(0)}
{-F_n''(0) - \frac 1 2 \tilde \lambda_n F_n''' (\eta_n)}
\end{split}
$$

We can find $F_n'(0)$ and $F_n''(0)$ by taking the derivatives with respect to $\lambda$ and evaluating at $\lambda = 0$.

$$
\begin{split}
F_n'(0) & = \frac {\partial}{\partial \lambda}\left[ \frac
{\sum_{i=1}^{n-p} u^2}
{\sum_{j=0}^m f_j \frac{1}{1+\lambda \phi_j}U_j + \sum_{s\in \Omega_0} \frac{1}{1+\lambda \rho_s}u_s^2} + \frac 1 n \sum_{j=1}^m f_j \log\left(\frac{1}{1+\lambda \phi_j}\right)\right]_{\lambda = 0} \\
& = \frac
{\sum_{j=0}^m \frac{f_j}{n-p} \phi_j U_j + \sum_{s\in \Omega_0} \frac{\rho_s u_s^2}{n-p}}
{\sum_{j=0}^m \frac{f_j'}{n-p}U_j + \sum_{s\in \Omega_0} \frac{u_s^2}{n-p}} - \sum_{j=1}^m \frac {f_j}{n} \phi_j \\
F_n''(0) & = \left(
\frac
{\sum_{j=0}^m \frac{f_j}{n-p} \phi_j U_j + \sum_{s\in \Omega_0} \frac{\rho_s u_s^2}{n-p}}
{\sum_{j=0}^m \frac{f_j'}{n-p}U_j + \sum_{s\in \Omega_0} \frac{u_s^2}{n-p}}
\right) ^2 
- \frac
{2 \sum_{j=0}^m \frac{f_j'}{n-p}\phi_j^2U_j + 2 \sum_{s\in \Omega_0} \frac{\rho_s^2u_s^2}{n-p}}
{\sum_{j-0}^m \frac{f_j'}{n-p}U_j + \sum_{s\in \Omega_0}\frac{u_s^2}{n-p}}
+\sum_{j=1}^m \frac{f_j}{n} \phi^2_j
\end{split}
$$

Let $\bar \rho_n = \sum_{j=0}^m \frac{f_j'}{n-p} \phi_j$ and $\bar \zeta_n = \sum_{j=0}^m \frac{f_j}{n}\phi_j$. 

We can now find what the different parts of the equation converge to.

$$
\begin{split}
\sqrt{n-p} F_n'(0) & = \sqrt{n-p}\left(
\frac
{\sum_{j=0}^m \frac{f_j'}{n-p}\phi_j U_j + O\left(\frac{p}{n-p}\right)}
{\sum_{j=0}^m \frac{f_j'}{n-p}U_j + O\left(\frac{p}{n-p}\right)} - \bar \zeta_n
\right) \\
& = \frac
{\sum_{j=0}^m \sqrt{\frac{f_j'}{n-p}}(\phi_j - \bar \zeta_n)\sqrt{f_j'}(U_j - 1) +\sqrt{n-p}(\bar \rho_n - \bar \zeta_n) + O\left(\frac{p}{\sqrt{n-p}}\right)}
{\sum_{j=0}^m \frac{f_j'}{n-p}U_j + O\left(\frac{p}{n-p}\right)}
\end{split}
$$

Here we note that $\bar \rho_n \rightarrow \sum_{j=0}^m q_j \phi_j = \bar \phi$ and $\bar \zeta_n \rightarrow \bar \phi$. 

Additionally, we note the following:

$$
\begin{split}
\sqrt{f_j'}(U_j - 1) \rightarrow^D z_i = N(0,2) \\
\sum_{j=0}^m \frac{f_j'}{n-p} U_j + O\left(\frac{p}{n-p}\right) \rightarrow^p 1
\end{split}
$$

Also, let $u^* = \lim_{n\rightarrow+\infty} \sqrt{n-p} (\bar \rho_n - \bar \zeta_n) = \lim_{n\rightarrow+\infty} u_n = 0$. Recall that in the paper $V(Z_\phi) = \sum_{j=0}^m q_j(\phi_j - 1)^2$

Under those conditions and by Slutsky's theorem

$$
\sqrt{n-p} F_n'(0) \rightarrow^D N(u^*, 2V(Z_\phi))
$$

For the second part of the equation, note that:

$$
F_n''(0) \rightarrow^p F_n''(\lambda_0) = -V(Z_\phi)
$$

We note that $F_n'''(\eta_n)$ is finite and the $\hat \lambda_n \rightarrow^p 0$ as defined by Lemma 3. This implies that $\hat \lambda_n F_n'''(\eta_n)\rightarrow^p 0$.

This lets us say that

$$
\sqrt{n-p} \tilde \lambda_n \rightarrow^D N\left(0, \frac{2}{V(Z_\phi)}\right)
$$

And since this hypothesis test is bounded on the edge of the parameter space, we can say that:

$$
\sqrt{n-p} \hat \lambda_n \rightarrow^D \frac 1 2 (1) + \frac 1 2 N\left(0, \frac{2}{V(Z_\phi)}\right)
$$

**ii)**

# Simulation Study

# Discussion




